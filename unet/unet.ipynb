{"cells":[{"cell_type":"markdown","metadata":{"id":"bRI7pD_UCyTg"},"source":["# **Binary Hands Segmentation**\n","\n",">***Notebook sections:***\n","1. Dataset\n","2. Unet model\n","3. Training phase\n","4. Testing phase\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kjOQ_Zw08Pai","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659113822619,"user_tz":-120,"elapsed":2000,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}},"outputId":"a1626d38-10c8-409c-b042-52c83a70bdc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"1VVLYYVmSCsh"},"source":["***Program parameters***"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"b9myUpO59V6A","executionInfo":{"status":"ok","timestamp":1659113822620,"user_tz":-120,"elapsed":7,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}}},"outputs":[],"source":["IMAGE_SIZE = (384, 224) # Images dimension in input of the DNN\n","BATCH_SIZE = 20\n","INPUT_CHANNELS = 3\n","PROJECT_DIR = '/content/drive/MyDrive/Uni/CV Project/'\n","CHECKPOINT_PATH = PROJECT_DIR + f'Alberto/unet/checkpoints/checkpoint_{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}/checkpoint.ckpt'\n","SAVE_MODEL_PATH = PROJECT_DIR + f'Alberto/unet/saved_models/unet_{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}'\n","NUM_IMAGES = None # Can't be greater than the size of the dataset; \"None\" for using all the images"]},{"cell_type":"markdown","metadata":{"id":"_yox9-2qrL38"},"source":["# **1. Dataset**"]},{"cell_type":"markdown","metadata":{"id":"ELEN9-bjUo1A"},"source":["***Datasets import***"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"b18EzOYjyvTy","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["7ba442be3fa3487d9032f93eb4f5631d","9ad6c0c2e1f64891995531461ea7b450","da3a866dadec48dd8aa2a5c3ebbc4b43","9e69d578081f4b92bcb3bbe0f995485d","77c746fb08e340a69b985b6faf451af7","2738f95c65fd4ccea6210f1098ed5eec","3ec61fb99cda4c2d9de4ee93580681bd","f7607167637e4c89b10887f492a633c9","7663b7cc21b4487fa4798319e0747a27","77db55bf603a44ceb349127598d77eef","916095ff00af4635af95d1c38fd8ae28"]},"executionInfo":{"status":"ok","timestamp":1659113826545,"user_tz":-120,"elapsed":3932,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}},"outputId":"c2f8f63b-eec9-488b-fcce-d1ca02918639"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ba442be3fa3487d9032f93eb4f5631d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Total number of usable images:  5068\n"]}],"source":["from tqdm.autonotebook import tqdm\n","import glob\n","\n","dataset_names = [\"egohands\", \n","                 #\"handsoverface\", \n","                 #\"eyth\",\n","                 \"handsoverface_prof\",\n","                 #\"egtea\"\n","                 ]\n","\n","# Getting images and masks filenames\n","data_images = []\n","data_masks = []\n","for dataset in tqdm(dataset_names):\n","  data_images = data_images + sorted(glob.glob(PROJECT_DIR + \"datasets/\" + dataset + \"/DATA_IMAGES/*\"))\n","  data_masks = data_masks + sorted(glob.glob(PROJECT_DIR + \"datasets/\" + dataset + \"/DATA_MASKS/*\"))\n","\n","print(\"Total number of usable images: \", len(data_images))"]},{"cell_type":"markdown","metadata":{"id":"1FUVG0e8UupQ"},"source":["***Filenames processing***"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SohtAQvHwvq5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659113826546,"user_tz":-120,"elapsed":11,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}},"outputId":"63e793bb-edb2-4737-feec-330c4c2d6677"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traininig set size:  4562\n","Validation set size:  506\n"]}],"source":["import random\n","import os\n","\n","# Shuffling data\n","c = list(zip(data_images, data_masks))\n","random.shuffle(c)\n","data_images, data_masks = zip(*c)\n","data_images = list(data_images)\n","data_masks = list(data_masks)\n","\n","# Simple check\n","assert len(data_images) == len(data_masks)\n","\n","N_TOTAL_IMAGES = len(data_images)\n","\n","if NUM_IMAGES is None:\n","  NUM_TRAIN_IMAGES = N_TOTAL_IMAGES - int(N_TOTAL_IMAGES * 10/100)\n","  NUM_VAL_IMAGES = N_TOTAL_IMAGES - NUM_TRAIN_IMAGES\n","else:\n","  NUM_TRAIN_IMAGES = NUM_IMAGES - int(NUM_IMAGES * 10/100)\n","  NUM_VAL_IMAGES = NUM_IMAGES - NUM_TRAIN_IMAGES\n","\n","# Splitting data into train/val set\n","train_images = data_images[:NUM_TRAIN_IMAGES]\n","train_masks = data_masks[:NUM_TRAIN_IMAGES]\n","val_images = data_images[NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES]\n","val_masks = data_masks[NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES]\n","\n","print(\"Traininig set size: \", len(train_images))\n","print(\"Validation set size: \", len(val_images))"]},{"cell_type":"markdown","metadata":{"id":"BY5ZelMmRjmu"},"source":["***DataGenerator class definition***\n","\n","* [Code reference](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) (by Afshine Amidi and Shervine Amidi)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Wv-o94P3Z1MR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659113829593,"user_tz":-120,"elapsed":3055,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}},"outputId":"c9537063-f57a-4098-b216-b7dabf1615a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.8.2\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","print(f\"TensorFlow version: {tf.__version__}\")\n","import cv2\n","\n","class DataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"\n","    Generates data for Keras\n","    Sequence based data generator. Suitable for building data generator for training and prediction.\n","    \"\"\"\n","    def __init__(self, image_paths, mask_paths,\n","                 to_fit=True, batch_size=16, dim=(512, 288),\n","                 n_channels=3, n_classes=1, shuffle=True):\n","        \"\"\"\n","        Initialization\n","        :param image_paths: path of images\n","        :param mask_paths: paths of masks\n","        :param to_fit: True to return X and y, False to return X only\n","        :param batch_size: batch size at each iteration\n","        :param dim: tuple indicating image dimension\n","        :param n_channels: number of image channels\n","        :param n_classes: number of output masks\n","        :param shuffle: True to shuffle label indexes after every epoch\n","        \"\"\"\n","        self.image_paths = image_paths\n","        self.mask_paths = mask_paths\n","        self.to_fit = to_fit\n","        self.batch_size = batch_size\n","        self.dim = dim\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        \"\"\"\n","        Denotes the number of batches per epoch\n","        :return: number of batches per epoch\n","        \"\"\"\n","        return int(np.floor(len(self.image_paths) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Generate one batch of data\n","        :param index: index of the batch\n","        :return: X and y when fitting. X only when predicting\n","        \"\"\"\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n","\n","        images_ids = []\n","        masks_ids = []\n","        for i in range(0, len(indexes)):\n","          images_ids.append(train_images[indexes[i]])\n","          masks_ids.append(train_masks[indexes[i]])\n","\n","        # Generate data\n","        X = self._generate_X(images_ids)\n","\n","        if self.to_fit:\n","            y = self._generate_y(masks_ids)\n","            return X, y\n","        else:\n","            return X\n","\n","    def on_epoch_end(self):\n","        \"\"\"\n","        Updates indexes after each epoch\n","        \"\"\"\n","        self.indexes = np.arange(len(self.image_paths))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def _generate_X(self, list_IDs_temp):\n","        \"\"\"\n","        Generates data containing batch_size images\n","        :param list_IDs_temp: list of images ids to load\n","        :return: batch of images\n","        \"\"\"\n","        images = []\n","        if self.n_channels == 3:\n","          images = [cv2.imread(img, cv2.IMREAD_COLOR) for img in list_IDs_temp]\n","        elif self.n_channels == 1:\n","          images = [cv2.imread(img, cv2.IMREAD_GRAYSCALE) for img in list_IDs_temp]\n","        else:\n","          raise Exception(\"Number of possibile input channels: 1, 3\")\n","        for i in range(0, len(images)):\n","          images[i] = cv2.resize(images[i], IMAGE_SIZE)\n","        image_dataset = np.array(images)\n","        return image_dataset / 255\n","\n","    def _generate_y(self, list_IDs_temp):\n","        \"\"\"\n","        Generates data containing batch_size masks\n","        :param list_IDs_temp: list of masks ids to load\n","        :return: batch of masks\n","        \"\"\"\n","        masks = [cv2.imread(mask, cv2.IMREAD_GRAYSCALE) for mask in list_IDs_temp]\n","        for i in range(0, len(masks)):\n","          masks[i] = cv2.resize(masks[i], IMAGE_SIZE, interpolation = cv2.INTER_NEAREST)\n","        mask_dataset = np.array(masks)\n","        mask_dataset = np.expand_dims(mask_dataset, axis = 3)\n","        return mask_dataset / 255"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ieYNZ4VyaNff","executionInfo":{"status":"ok","timestamp":1659113829594,"user_tz":-120,"elapsed":4,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}}},"outputs":[],"source":["# Creating data generators for the training phase\n","train_generator = DataGenerator(train_images, train_masks, batch_size=BATCH_SIZE, dim=IMAGE_SIZE,\n","                 n_channels=INPUT_CHANNELS, shuffle=True)\n","val_generator = DataGenerator(val_images, val_masks, batch_size=BATCH_SIZE, dim=IMAGE_SIZE,\n","                 n_channels=INPUT_CHANNELS, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"Vr4ARXTXrITf"},"source":["# **2. Unet model**"]},{"cell_type":"markdown","metadata":{"id":"5HCsnjPJU4n5"},"source":["***Unet definition*** \n","\n","* [Code reference]( https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial117_building_unet_using_encoder_decoder_blocks.ipynb) (by Dr. Sreenivas Bhattiprolu)\n","* [Paper reference](https://arxiv.org/abs/1505.04597) (U-Net: Convolutional Networks for Biomedical Image Segmentation)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9u81j7HFTEKs","executionInfo":{"status":"ok","timestamp":1659113829594,"user_tz":-120,"elapsed":3,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}}},"outputs":[],"source":["from tensorflow import keras\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n","from keras.layers import Activation, MaxPool2D, Concatenate\n","\n","def conv_block(input, num_filters):\n","    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","# Encoder block: Conv block followed by maxpooling\n","def encoder_block(input, num_filters):\n","    x = conv_block(input, num_filters)\n","    p = MaxPool2D((2, 2))(x)\n","    return x, p   \n","\n","# Decoder block\n","# skip_features gets input from encoder for concatenation\n","def decoder_block(input, skip_features, num_filters):\n","    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n","    x = Concatenate()([x, skip_features])\n","    x = conv_block(x, num_filters)\n","    return x\n","\n","# Build Unet using the blocks\n","def build_unet(input_shape, n_classes):\n","    inputs = Input(input_shape)\n","\n","    s1, p1 = encoder_block(inputs, 64)\n","    s2, p2 = encoder_block(p1, 128)\n","    s3, p3 = encoder_block(p2, 256)\n","    s4, p4 = encoder_block(p3, 512)\n","\n","    b1 = conv_block(p4, 1024) #Bridge\n","\n","    d1 = decoder_block(b1, s4, 512)\n","    d2 = decoder_block(d1, s3, 256)\n","    d3 = decoder_block(d2, s2, 128)\n","    d4 = decoder_block(d3, s1, 64)\n","\n","    if n_classes == 1:  #Binary\n","      activation = 'sigmoid'\n","    else:\n","      activation = 'softmax'\n","\n","    # Change the activation based on n_classes\n","    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)\n","\n","    model = Model(inputs, outputs, name=\"U-Net\")\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"4CV5taukIxIW"},"source":["***Unet import***"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ms4yuNMxTK1o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659113834104,"user_tz":-120,"elapsed":4513,"user":{"displayName":"lettomobile","userId":"05123550173110449334"}},"outputId":"56bb8274-2476-4f86-8440-5e6f7b3d7344"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"U-Net\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 224, 384, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 224, 384, 64  1792        ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 224, 384, 64  256        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 224, 384, 64  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 224, 384, 64  36928       ['activation[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 224, 384, 64  256        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_1 (Activation)      (None, 224, 384, 64  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 112, 192, 64  0           ['activation_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 112, 192, 12  73856       ['max_pooling2d[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 112, 192, 12  512        ['conv2d_2[0][0]']               \n"," rmalization)                   8)                                                                \n","                                                                                                  \n"," activation_2 (Activation)      (None, 112, 192, 12  0           ['batch_normalization_2[0][0]']  \n","                                8)                                                                \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 112, 192, 12  147584      ['activation_2[0][0]']           \n","                                8)                                                                \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 112, 192, 12  512        ['conv2d_3[0][0]']               \n"," rmalization)                   8)                                                                \n","                                                                                                  \n"," activation_3 (Activation)      (None, 112, 192, 12  0           ['batch_normalization_3[0][0]']  \n","                                8)                                                                \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 56, 96, 128)  0          ['activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 56, 96, 256)  295168      ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 56, 96, 256)  1024       ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 56, 96, 256)  0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 56, 96, 256)  590080      ['activation_4[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 56, 96, 256)  1024       ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 56, 96, 256)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 28, 48, 256)  0          ['activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 28, 48, 512)  1180160     ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 28, 48, 512)  2048       ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 28, 48, 512)  0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 28, 48, 512)  2359808     ['activation_6[0][0]']           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 28, 48, 512)  2048       ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_7 (Activation)      (None, 28, 48, 512)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 14, 24, 512)  0          ['activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 14, 24, 1024  4719616     ['max_pooling2d_3[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 14, 24, 1024  4096       ['conv2d_8[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_8 (Activation)      (None, 14, 24, 1024  0           ['batch_normalization_8[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 14, 24, 1024  9438208     ['activation_8[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 14, 24, 1024  4096       ['conv2d_9[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," activation_9 (Activation)      (None, 14, 24, 1024  0           ['batch_normalization_9[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_transpose (Conv2DTransp  (None, 28, 48, 512)  2097664    ['activation_9[0][0]']           \n"," ose)                                                                                             \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 28, 48, 1024  0           ['conv2d_transpose[0][0]',       \n","                                )                                 'activation_7[0][0]']           \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 28, 48, 512)  4719104     ['concatenate[0][0]']            \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 28, 48, 512)  2048       ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 28, 48, 512)  0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 28, 48, 512)  2359808     ['activation_10[0][0]']          \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 28, 48, 512)  2048       ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 28, 48, 512)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," conv2d_transpose_1 (Conv2DTran  (None, 56, 96, 256)  524544     ['activation_11[0][0]']          \n"," spose)                                                                                           \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 56, 96, 512)  0           ['conv2d_transpose_1[0][0]',     \n","                                                                  'activation_5[0][0]']           \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 56, 96, 256)  1179904     ['concatenate_1[0][0]']          \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 56, 96, 256)  1024       ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_12 (Activation)     (None, 56, 96, 256)  0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 56, 96, 256)  590080      ['activation_12[0][0]']          \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 56, 96, 256)  1024       ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_13 (Activation)     (None, 56, 96, 256)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," conv2d_transpose_2 (Conv2DTran  (None, 112, 192, 12  131200     ['activation_13[0][0]']          \n"," spose)                         8)                                                                \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 112, 192, 25  0           ['conv2d_transpose_2[0][0]',     \n","                                6)                                'activation_3[0][0]']           \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 112, 192, 12  295040      ['concatenate_2[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 112, 192, 12  512        ['conv2d_14[0][0]']              \n"," ormalization)                  8)                                                                \n","                                                                                                  \n"," activation_14 (Activation)     (None, 112, 192, 12  0           ['batch_normalization_14[0][0]'] \n","                                8)                                                                \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 112, 192, 12  147584      ['activation_14[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 112, 192, 12  512        ['conv2d_15[0][0]']              \n"," ormalization)                  8)                                                                \n","                                                                                                  \n"," activation_15 (Activation)     (None, 112, 192, 12  0           ['batch_normalization_15[0][0]'] \n","                                8)                                                                \n","                                                                                                  \n"," conv2d_transpose_3 (Conv2DTran  (None, 224, 384, 64  32832      ['activation_15[0][0]']          \n"," spose)                         )                                                                 \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 224, 384, 12  0           ['conv2d_transpose_3[0][0]',     \n","                                8)                                'activation_1[0][0]']           \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 224, 384, 64  73792       ['concatenate_3[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 224, 384, 64  256        ['conv2d_16[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_16 (Activation)     (None, 224, 384, 64  0           ['batch_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 224, 384, 64  36928       ['activation_16[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 224, 384, 64  256        ['conv2d_17[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_17 (Activation)     (None, 224, 384, 64  0           ['batch_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 224, 384, 1)  65          ['activation_17[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 31,055,297\n","Trainable params: 31,043,521\n","Non-trainable params: 11,776\n","__________________________________________________________________________________________________\n"]}],"source":["import os\n","\n","input_shape = (IMAGE_SIZE[1], IMAGE_SIZE[0], INPUT_CHANNELS)\n","model = build_unet(input_shape, 1) # 1 for binary segmentation\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n","              loss='binary_crossentropy', \n","              metrics=['accuracy'])\n","if os.path.exists(CHECKPOINT_PATH):\n","  model.load_weights(CHECKPOINT_PATH)\n","  print(\"Checkpoint found: weights loaded\")\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"uWvlQqhMrElc"},"source":["# **3. Training phase**"]},{"cell_type":"markdown","metadata":{"id":"wGduMc7IV2Vu"},"source":["***Training start***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sL6B2k7MnEwV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f76ea6bd-787a-4b96-fbef-230a3687ed22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","228/228 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9488 \n","Epoch 1: val_loss improved from inf to 0.78969, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 3206s 14s/step - loss: 0.1324 - accuracy: 0.9488 - val_loss: 0.7897 - val_accuracy: 0.7537\n","Epoch 2/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9648\n","Epoch 2: val_loss improved from 0.78969 to 0.13887, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 177s 775ms/step - loss: 0.0862 - accuracy: 0.9648 - val_loss: 0.1389 - val_accuracy: 0.9470\n","Epoch 3/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9693\n","Epoch 3: val_loss improved from 0.13887 to 0.08123, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 176s 771ms/step - loss: 0.0763 - accuracy: 0.9693 - val_loss: 0.0812 - val_accuracy: 0.9679\n","Epoch 4/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9722\n","Epoch 4: val_loss did not improve from 0.08123\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0691 - accuracy: 0.9722 - val_loss: 0.0861 - val_accuracy: 0.9667\n","Epoch 5/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9741\n","Epoch 5: val_loss did not improve from 0.08123\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0639 - accuracy: 0.9741 - val_loss: 0.0953 - val_accuracy: 0.9627\n","Epoch 6/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9762\n","Epoch 6: val_loss did not improve from 0.08123\n","228/228 [==============================] - 175s 765ms/step - loss: 0.0588 - accuracy: 0.9762 - val_loss: 0.0844 - val_accuracy: 0.9696\n","Epoch 7/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9777\n","Epoch 7: val_loss did not improve from 0.08123\n","228/228 [==============================] - 175s 765ms/step - loss: 0.0551 - accuracy: 0.9777 - val_loss: 0.1144 - val_accuracy: 0.9573\n","Epoch 8/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9786\n","Epoch 8: val_loss improved from 0.08123 to 0.06166, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 176s 771ms/step - loss: 0.0525 - accuracy: 0.9786 - val_loss: 0.0617 - val_accuracy: 0.9749\n","Epoch 9/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9798\n","Epoch 9: val_loss did not improve from 0.06166\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0487 - accuracy: 0.9798 - val_loss: 0.0791 - val_accuracy: 0.9698\n","Epoch 10/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9802\n","Epoch 10: val_loss did not improve from 0.06166\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0477 - accuracy: 0.9802 - val_loss: 0.1353 - val_accuracy: 0.9548\n","Epoch 11/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9810\n","Epoch 11: val_loss improved from 0.06166 to 0.04996, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 177s 774ms/step - loss: 0.0461 - accuracy: 0.9810 - val_loss: 0.0500 - val_accuracy: 0.9794\n","Epoch 12/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9820\n","Epoch 12: val_loss did not improve from 0.04996\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0434 - accuracy: 0.9820 - val_loss: 0.0822 - val_accuracy: 0.9695\n","Epoch 13/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9830\n","Epoch 13: val_loss did not improve from 0.04996\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0412 - accuracy: 0.9830 - val_loss: 0.0500 - val_accuracy: 0.9788\n","Epoch 14/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9834\n","Epoch 14: val_loss did not improve from 0.04996\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0398 - accuracy: 0.9834 - val_loss: 0.0539 - val_accuracy: 0.9788\n","Epoch 15/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9838\n","Epoch 15: val_loss did not improve from 0.04996\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0392 - accuracy: 0.9838 - val_loss: 0.0624 - val_accuracy: 0.9749\n","Epoch 16/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9844\n","Epoch 16: val_loss improved from 0.04996 to 0.03531, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 177s 775ms/step - loss: 0.0373 - accuracy: 0.9844 - val_loss: 0.0353 - val_accuracy: 0.9852\n","Epoch 17/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9850\n","Epoch 17: val_loss did not improve from 0.03531\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0361 - accuracy: 0.9850 - val_loss: 0.0414 - val_accuracy: 0.9828\n","Epoch 18/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9859\n","Epoch 18: val_loss did not improve from 0.03531\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0338 - accuracy: 0.9859 - val_loss: 0.0542 - val_accuracy: 0.9782\n","Epoch 19/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9851\n","Epoch 19: val_loss did not improve from 0.03531\n","228/228 [==============================] - 175s 765ms/step - loss: 0.0360 - accuracy: 0.9851 - val_loss: 0.0417 - val_accuracy: 0.9829\n","Epoch 20/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9868\n","Epoch 20: val_loss improved from 0.03531 to 0.03201, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 177s 773ms/step - loss: 0.0313 - accuracy: 0.9868 - val_loss: 0.0320 - val_accuracy: 0.9866\n","Epoch 21/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9871\n","Epoch 21: val_loss did not improve from 0.03201\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0306 - accuracy: 0.9871 - val_loss: 0.0344 - val_accuracy: 0.9855\n","Epoch 22/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9873\n","Epoch 22: val_loss improved from 0.03201 to 0.03065, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 177s 774ms/step - loss: 0.0302 - accuracy: 0.9873 - val_loss: 0.0306 - val_accuracy: 0.9871\n","Epoch 23/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9869\n","Epoch 23: val_loss did not improve from 0.03065\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0311 - accuracy: 0.9869 - val_loss: 0.0400 - val_accuracy: 0.9841\n","Epoch 24/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9868\n","Epoch 24: val_loss did not improve from 0.03065\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0317 - accuracy: 0.9868 - val_loss: 0.1711 - val_accuracy: 0.9268\n","Epoch 25/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9857\n","Epoch 25: val_loss did not improve from 0.03065\n","228/228 [==============================] - 175s 766ms/step - loss: 0.0343 - accuracy: 0.9857 - val_loss: 0.0310 - val_accuracy: 0.9869\n","Epoch 26/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9879\n","Epoch 26: val_loss did not improve from 0.03065\n","228/228 [==============================] - 175s 767ms/step - loss: 0.0286 - accuracy: 0.9879 - val_loss: 0.0312 - val_accuracy: 0.9870\n","Epoch 27/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9885\n","Epoch 27: val_loss improved from 0.03065 to 0.02732, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 177s 774ms/step - loss: 0.0271 - accuracy: 0.9885 - val_loss: 0.0273 - val_accuracy: 0.9884\n","Epoch 28/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9885\n","Epoch 28: val_loss improved from 0.02732 to 0.02618, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 176s 773ms/step - loss: 0.0271 - accuracy: 0.9885 - val_loss: 0.0262 - val_accuracy: 0.9888\n","Epoch 29/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9880\n","Epoch 29: val_loss did not improve from 0.02618\n","228/228 [==============================] - 175s 767ms/step - loss: 0.0284 - accuracy: 0.9880 - val_loss: 0.0270 - val_accuracy: 0.9885\n","Epoch 30/50\n","228/228 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9890\n","Epoch 30: val_loss improved from 0.02618 to 0.02499, saving model to /content/drive/MyDrive/Uni/CV Project/Alberto/unet/checkpoints/checkpoint_384x224/checkpoint.ckpt\n","228/228 [==============================] - 176s 773ms/step - loss: 0.0258 - accuracy: 0.9890 - val_loss: 0.0250 - val_accuracy: 0.9894\n","Epoch 31/50\n","167/228 [====================>.........] - ETA: 43s - loss: 0.0240 - accuracy: 0.9897"]}],"source":["# Defining some callbacks\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=CHECKPOINT_PATH,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True,\n","    verbose=1,\n","    )\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    patience=5\n","    )\n","\n","# Training the model\n","history = model.fit(train_generator, \n","                    validation_data=val_generator,\n","                    verbose=1,\n","                    epochs=50,\n","                    callbacks=[model_checkpoint, \n","                               early_stopping])\n","\n","model.save(SAVE_MODEL_PATH)\n","print(\"Model saved\")"]},{"cell_type":"markdown","metadata":{"id":"noUTiFNtV53c"},"source":["***Training stats***\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8tAV2nApiEK"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","# Plotting loss/val_loss for each epoch\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(loss) + 1)\n","plt.plot(epochs, loss, 'y', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Plotting accuracies for each epoch\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","plt.plot(epochs, acc, 'y', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation acc')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"KjkaUNHJGbKY"},"source":["# **4. Testing phase**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxYMJDPUplbF"},"outputs":[],"source":["from keras.models import load_model\n","\n","# Loading model from the last checkpoint saved\n","model = load_model(SAVE_MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTGWOJnb034I"},"outputs":[],"source":["import cv2\n","\n","def preprocess_image(image_path, resize_dim):\n","  \"\"\"\n","  Transforms an image in order to be usable by the DNN.\n","  It makes the same transformations you can find in the DataGenerator class.\n","  \"\"\"\n","  if INPUT_CHANNELS == 3:\n","    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","  elif INPUT_CHANNELS == 1:\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","  else:\n","    raise Exception(\"Number of possibile input channels: 1, 3\")\n","  image = cv2.resize(image, resize_dim)\n","  return image, image / 255"]},{"cell_type":"markdown","metadata":{"id":"UKhNrQCoW9Bc"},"source":["***Results on test set***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tEUAYfIGiU1"},"outputs":[],"source":["import glob\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","images_name = glob.glob(PROJECT_DIR + \"/test_set/*\")\n","threshold = 0.5\n","\n","for i in range(0, len(images_name)):\n","  # Reading an image from the test set\n","  image255, image = preprocess_image(images_name[i], IMAGE_SIZE)  \n","\n","  image = np.expand_dims(image, 0)\n","  print(image.shape)\n","  print(image.type())\n","\n","  # Predicting the mask\n","  prediction = (model.predict(image)[0,:,:,0] > threshold).astype(np.uint8)\n","\n","  # Showing the image and its predicted mask\n","  plt.figure(figsize=(16, 8))\n","  plt.subplot(231)\n","  plt.title('Testing Image')\n","  plt.imshow(cv2.cvtColor(image255, cv2.COLOR_BGR2RGB))\n","  plt.subplot(232)\n","  plt.title('Prediction on test image')\n","  plt.imshow(prediction, cmap='gray')\n","  plt.show()\n","  break"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"unet.ipynb","provenance":[{"file_id":"1LvRJ7cW11s9EvziKgtydNmxdl8fAi78V","timestamp":1657325161598}],"mount_file_id":"11Tef1HODagqDoTL4Fifv7l4Y4GOpPZUt","authorship_tag":"ABX9TyOt4Sx6yNV+fO6CLYLV9KWw"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7ba442be3fa3487d9032f93eb4f5631d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ad6c0c2e1f64891995531461ea7b450","IPY_MODEL_da3a866dadec48dd8aa2a5c3ebbc4b43","IPY_MODEL_9e69d578081f4b92bcb3bbe0f995485d"],"layout":"IPY_MODEL_77c746fb08e340a69b985b6faf451af7"}},"9ad6c0c2e1f64891995531461ea7b450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2738f95c65fd4ccea6210f1098ed5eec","placeholder":"​","style":"IPY_MODEL_3ec61fb99cda4c2d9de4ee93580681bd","value":"100%"}},"da3a866dadec48dd8aa2a5c3ebbc4b43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7607167637e4c89b10887f492a633c9","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7663b7cc21b4487fa4798319e0747a27","value":2}},"9e69d578081f4b92bcb3bbe0f995485d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77db55bf603a44ceb349127598d77eef","placeholder":"​","style":"IPY_MODEL_916095ff00af4635af95d1c38fd8ae28","value":" 2/2 [00:04&lt;00:00,  2.36s/it]"}},"77c746fb08e340a69b985b6faf451af7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2738f95c65fd4ccea6210f1098ed5eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec61fb99cda4c2d9de4ee93580681bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7607167637e4c89b10887f492a633c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7663b7cc21b4487fa4798319e0747a27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77db55bf603a44ceb349127598d77eef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"916095ff00af4635af95d1c38fd8ae28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}